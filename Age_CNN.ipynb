{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lauralethia/RedesAge/blob/master/Age_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcweBPFVNyIF",
        "colab_type": "text"
      },
      "source": [
        "CNN para predecir edad a partir de volúmenes MRI-T1\n",
        "Noviembre 2019\n",
        "Laura Alethia de la Fuente & Mauro Namías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDFb77pDOCu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy import ndimage as nd\n",
        "import sys\n",
        "\n",
        "#import imageio # To save images as gif\n",
        "import torch.utils.data as d_u\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEF-iItXXytR",
        "colab_type": "text"
      },
      "source": [
        "Definición de la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z_6tJ5CXNkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv3d(1, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv3d(8, 8, 3, stride = 2, padding=0)\n",
        "        \n",
        "        self.conv3 = nn.Conv3d(8, 16, 3, padding=1)\n",
        "        self.conv4 = nn.Conv3d(16, 16, 3, stride = 2, padding=0)\n",
        "        \n",
        "        self.conv5 = nn.Conv3d(16, 32, 3, padding=1)\n",
        "        self.conv6 = nn.Conv3d(32, 32, 3, padding=1)\n",
        "        self.conv7 = nn.Conv3d(32, 32, 3, stride =2 ,padding=0)\n",
        "        \n",
        "        self.conv8 = nn.Conv3d(32, 64, 3, padding=1)\n",
        "        self.conv9 = nn.Conv3d(64, 64, 3, padding=1)\n",
        "        self.conv10 = nn.Conv3d(64, 64, 3, stride = 2, padding=0)\n",
        "      \n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128,momentum=0.1)\n",
        "        self.do = nn.Dropout(p = 0.7, inplace=False)        \n",
        "        self.fc2 = nn.Linear(128, 64)        \n",
        "        self.fc3 = nn.Linear(64,1)\n",
        "        \n",
        "     \n",
        "   \n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.elu(self.conv1(x))\n",
        "        x = F.elu(self.conv2(x))        \n",
        "        x = F.elu(self.conv3(x))\n",
        "        x = F.elu(self.conv4(x))\n",
        "        x = F.elu(self.conv5(x))\n",
        "        x = F.elu(self.conv6(x))\n",
        "        x = F.elu(self.conv7(x))\n",
        "        x = F.elu(self.conv8(x))\n",
        "        x = F.elu(self.conv9(x))\n",
        "        x = F.elu(self.conv10(x))\n",
        "        \n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "#        print(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.elu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.do(x)        \n",
        "        x = self.fc2(x)\n",
        "        x = F.elu(x)\n",
        "        x = self.fc3(x) \n",
        "\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8qQQnIHX4Ga",
        "colab_type": "text"
      },
      "source": [
        "Probamos que haya quedado bien definida la red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1lzZVQEXiZJ",
        "colab_type": "code",
        "outputId": "cbc09342-8ce8-4870-f93f-c9595427c79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "## Inic.\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv2): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
            "  (conv3): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv4): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
            "  (conv5): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv6): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv7): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
            "  (conv8): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv9): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv10): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (do): Dropout(p=0.7, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KksSFDEyYKUq",
        "colab_type": "text"
      },
      "source": [
        "Probamos la red en CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-47DjIvGYJKi",
        "colab_type": "code",
        "outputId": "8369906a-75e5-466b-fb65-6183d074169c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = Variable(torch.randn(1,1, 46,52,40))\n",
        "#print(input)\n",
        "net.eval()\n",
        "output = net(input)\n",
        "print(output)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0274]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I13hhsgoZzpr",
        "colab_type": "text"
      },
      "source": [
        "Probamos la red en GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDO_3HUyZ2Vb",
        "colab_type": "code",
        "outputId": "a647d222-1f44-45e7-fc9e-dd086e07c5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "input = Variable(torch.randn(1,1, 46,52,40))\n",
        "net.eval()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "\n",
        "if use_cuda:\n",
        "    net = net.cuda()\n",
        "    input = input.cuda()\n",
        "\n",
        "tic = time.clock()\n",
        "output = net(input)\n",
        "print(output)\n",
        "toc = time.clock()\n",
        "print(toc-tic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "tensor([[0.0272]], grad_fn=<AddmmBackward>)\n",
            "0.07740599999999986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgwXSkvuzMi7",
        "colab_type": "code",
        "outputId": "c6e235e9-ab3a-4783-ec79-98a92b0953ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zE_WlceALsF",
        "colab_type": "code",
        "outputId": "1dfc2b04-aaa9-4751-85ad-2d73b3a8d45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# Load functions and check datasets\n",
        "\n",
        "#!ls \"/content/gdrive/My Drive/Doctorado/Age_CNN/data\"\n",
        "!ls \"/content/gdrive/My Drive/COCUCO/Redes/Datos\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AB_CALTECH_0051478_bet1_V.nii  AB_CMU_0050656_bet1_V.nii\n",
            "AB_CALTECH_0051479_bet1_V.nii  AB_CMU_0050657_bet1_V.nii\n",
            "AB_CALTECH_0051480_bet1_V.nii  AB_CMU_0050658_bet1_V.nii\n",
            "AB_CALTECH_0051481_bet1_V.nii  AB_CMU_0050659_bet1_V.nii\n",
            "AB_CALTECH_0051482_bet1_V.nii  AB_CMU_0050660_bet1_V.nii\n",
            "AB_CALTECH_0051483_bet1_V.nii  AB_CMU_0050661_bet1_V.nii\n",
            "AB_CALTECH_0051484_bet1_V.nii  AB_CMU_0050663_bet1_V.nii\n",
            "AB_CALTECH_0051485_bet1_V.nii  AB_CMU_0050664_bet1_V.nii\n",
            "AB_CALTECH_0051486_bet1_V.nii  AB_CMU_0050665_bet1_V.nii\n",
            "AB_CALTECH_0051487_bet1_V.nii  AB_CMU_0050666_bet1_V.nii\n",
            "AB_CALTECH_0051488_bet1_V.nii  AB_CMU_0050668_bet1_V.nii\n",
            "AB_CALTECH_0051489_bet1_V.nii  alldata_2c.csv\n",
            "AB_CALTECH_0051492_bet1_V.nii  reducidoColab.csv\n",
            "AB_CALTECH_0051493_bet1_V.nii\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxGYOrI-HI5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5YjGVMS39LU",
        "colab_type": "code",
        "outputId": "a2814f91-a65d-4e7d-f3e3-2a22d7da5d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Net parameters and files\n",
        "#!ls \"/content/drive/My Drive/COCUCO/Redes/Datos\"\n",
        "csvfDir = '/content/drive/My Drive/COCUCO/Redes/Datos'\n",
        "csvfName = 'reducido.csv'\n",
        "\n",
        "folds = 2\n",
        "nEpochs = 400\n",
        "bs = 20 # Bach size for training "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AB_CALTECH_0051478_bet1_V.nii  AB_CALTECH_0051493_bet1_V.nii\n",
            "AB_CALTECH_0051479_bet1_V.nii  AB_CMU_0050656_bet1_V.nii\n",
            "AB_CALTECH_0051480_bet1_V.nii  AB_CMU_0050657_bet1_V.nii\n",
            "AB_CALTECH_0051481_bet1_V.nii  AB_CMU_0050658_bet1_V.nii\n",
            "AB_CALTECH_0051482_bet1_V.nii  AB_CMU_0050659_bet1_V.nii\n",
            "AB_CALTECH_0051483_bet1_V.nii  AB_CMU_0050660_bet1_V.nii\n",
            "AB_CALTECH_0051484_bet1_V.nii  AB_CMU_0050661_bet1_V.nii\n",
            "AB_CALTECH_0051485_bet1_V.nii  AB_CMU_0050663_bet1_V.nii\n",
            "AB_CALTECH_0051486_bet1_V.nii  AB_CMU_0050664_bet1_V.nii\n",
            "AB_CALTECH_0051487_bet1_V.nii  AB_CMU_0050665_bet1_V.nii\n",
            "AB_CALTECH_0051488_bet1_V.nii  AB_CMU_0050666_bet1_V.nii\n",
            "AB_CALTECH_0051489_bet1_V.nii  AB_CMU_0050668_bet1_V.nii\n",
            "AB_CALTECH_0051492_bet1_V.nii  alldata_2c.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4ZlQ-7T94Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training and test run\n",
        "kfold_loss = []\n",
        "kfold_loss_t = []\n",
        "\n",
        "for k in range(folds):\n",
        "    net = Net()\n",
        "    net = net.cuda()\n",
        "    alldata = MRIDataset(csv_file=str(csvfDir+csvfName), root_dir=csvfDir)\n",
        "    tot = len(alldata)\n",
        "    alldataloader = DataLoader(alldata, batch_size=tot, shuffle=True, num_workers=0) \n",
        "    for i, data in enumerate(alldataloader):\n",
        "            inputs, labels = data\n",
        "            inputs = Variable(data[inputs].float(),requires_grad=True)\n",
        "            inputs = inputs.squeeze(1)\n",
        "            labels = Variable(data[labels].float())\n",
        "            labels = labels.squeeze(1)\n",
        "    \n",
        "    dataset_x = inputs.data.numpy()\n",
        "    dataset_y = labels.data.numpy().astype('float64')     \n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(dataset_x,dataset_y,test_size=0.33,\n",
        "                                                        random_state=42)    \n",
        "\n",
        "    my_sampler =d_u.sampler.SubsetRandomSampler(list(range(len(X_train))))\n",
        "    my_sampler2 =d_u.sampler.SubsetRandomSampler(list(range(len(X_test))))\n",
        "\n",
        "    X_train[X_train<0] = 0\n",
        "    X_test[X_test<0] = 0   \n",
        "    \n",
        "    test_largo = len(y_test)\n",
        "    train_largo = len(y_train)\n",
        "      \n",
        "  #samples_weight = np.ones([1,train_largo])\n",
        "  #weight = weight/weight.sum()\n",
        "  #samples_weight = np.array([weight[t] for t in y_train])\n",
        "  #samples_weight = samples_weight/3.0\n",
        "  #samples_weight = torch.from_numpy(samples_weight)  \n",
        "  # sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight),replacement=False)\n",
        "    trainDataset = d_u.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "    validDataset = d_u.TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test)) \n",
        "    del X_train, X_test\n",
        "    \n",
        "    bs_test = round(len(y_test)/2)\n",
        "    \n",
        "    trainLoader = d_u.DataLoader(dataset = trainDataset, batch_size=bs, num_workers=0, sampler = my_sampler)\n",
        "#    testLoader = d_u.DataLoader(dataset = validDataset, batch_size=bs_test, shuffle=True, num_workers=0) \n",
        "    testLoader = d_u.DataLoader(dataset = validDataset, batch_size=bs_test, num_workers=0,sampler = my_sampler2) \n",
        "   \n",
        "    net.train()\n",
        "       \n",
        "    lr = 0.01  # 0.1\n",
        "    wd = 0.01  # 0.01\n",
        "\n",
        "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr= lr ,momentum=0.1, weight_decay=wd)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "# One result figure by kfols\n",
        "    fig = plt.figure()\n",
        "    \n",
        "    batch_loss = []\n",
        "    batch_loss_t = []\n",
        "       \n",
        "    for epoch in range(nEpochs):  # loop over the dataset multiple times\n",
        "        print(\"learning rate\")\n",
        "        print(lr)\n",
        "        print(\"weight decay\")\n",
        "        print(wd)\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.1, weight_decay=wd)    \n",
        "        net.train()\n",
        "        mini_batch_loss = []\n",
        "        mini_batch_loss_t = []\n",
        "       \n",
        "        for i, data in enumerate(trainLoader):\n",
        "            inputs2,labels2 = data\n",
        "            dataumentada = dataugment(Variable(inputs2),Variable(labels2) )         \n",
        "            instanceloader = DataLoader(dataumentada, batch_size=len(labels), shuffle=False, num_workers=0)\n",
        "    \n",
        "            for i_a, data_a in enumerate(instanceloader):\n",
        "                inputs2, labels2 = data_a\n",
        "                inputs2 = data_a[inputs2]\n",
        "                labels2 = data_a[labels2]\n",
        "                \n",
        "            tici = time.clock()\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            inputs2 = Variable(inputs2)\n",
        "            labels2 = Variable(labels2).float()\n",
        "\n",
        "            outputs = net(inputs2.cuda())            \n",
        "\n",
        "            loss = criterion(outputs.cpu(),labels2)                   \n",
        "            loss = loss.cuda()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            mini_batch_loss.append(loss.data[0])\n",
        "        batch_loss.append(np.array(mini_batch_loss).mean())\n",
        "\n",
        "        toco = time.clock()\n",
        "    \n",
        "    #    torch.save(net.state_dict(), 'C:\\\\Users\\\\Amelie\\\\Documents\\\\Cursos\\\\Redes\\\\Proyecto\\\\Mau\\\\REDES\\\\T1ImgBet\\\\entrenadafullrot3.dat')\n",
        "    #    torch.save(net.state_dict(), 'C:\\\\Users\\\\Amelie\\\\Documents\\\\Cursos\\\\Redes\\\\Proyecto\\\\Mau\\\\REDES\\\\T1ImgVBM\\\\entrenada.dat')       \n",
        "        toc = time.clock()  \n",
        "    \n",
        "        print('Finished Epoch')\n",
        "        print(epoch)\n",
        "        \n",
        "        net.eval()\n",
        "        average_t_loss = 0\n",
        "                    \n",
        "        for i, data_t in enumerate(testLoader):\n",
        "                # get the inputs\n",
        "                tici = time.clock()\n",
        "                t_inputs2, t_labels2 = data_t\n",
        "                if t_labels2.size()[0] > 1:\n",
        "                    t_inputs2 = Variable(t_inputs2.float(),volatile = True)\n",
        "                    t_labels2 = Variable(t_labels2).float()\n",
        "            \n",
        "                    t_outputs = net(t_inputs2.cuda())\n",
        "                    t_loss = criterion(t_outputs.cpu(),t_labels2)\n",
        "                    mini_batch_loss_t.append(t_loss.data[0])\n",
        "        batch_loss_t.append(np.array(mini_batch_loss_t).mean())\n",
        "          \n",
        "# Result figures by fold          \n",
        "        plt.ion()\n",
        "        ax1 = fig.add_subplot(211)    # The big subplot\n",
        "        ax2 = fig.add_subplot(212)\n",
        "        \n",
        "        ax1.plot(batch_loss,color='y')\n",
        "        \n",
        "        x = np.array(batch_loss_t)\n",
        "        idx = [np.where(x >1000)]\n",
        "        x[idx] -= x[idx]\n",
        "        ax1.plot(x,color='g')\n",
        "        ax1.set_ylabel('Loss promedio train(y)/test(g)')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "\n",
        "        ax2.scatter(labels2.data.numpy(),outputs.cpu().data.numpy() ,color='red', alpha=1*(epoch/nEpochs))\n",
        "        ax2.scatter(t_labels2.data.numpy(),t_outputs.cpu().data.numpy() ,color='orange', alpha=1*(epoch/nEpochs))\n",
        "        ax2.set_ylabel('Dep/PredAge')\n",
        "        ax2.set_xlabel('Indep/Age [y = test ; r = train]')\n",
        "        ax2.set_ylim([15, 35])\n",
        "        ax2.set_xlim([15 ,35])\n",
        "        \n",
        "        fig.show()\n",
        "        plt.pause(0.05)\n",
        "  #          redfoldnale = 'C:\\\\Users\\\\Amelie\\\\Documents\\\\Alethianet\\\\Entrenada\\\\entrenada_aletheia_2019_drop08' +str(k) +'.dat'\n",
        "   #         torch.save(net.state_dict(), redfoldnale)\n",
        "#        print('average test MSE: ' + str(round((batch_loss_t),5)))\n",
        "        \n",
        "        toc = time.clock()\n",
        "        toc-tic\n",
        "        print('Finished Training')\n",
        "        print(toc-tic)\n",
        "    \n",
        " #   foldname = 'C:\\\\Users\\\\Amelie\\\\Documents\\\\Alethianet\\\\Entrenada\\\\' + 'trainfold_2019_drop08' + str(k)+ '.png'\n",
        "  #  plt.savefig(foldname)\n",
        "   # plt.close()\n",
        "kfold_loss.append(np.array(batch_loss).mean())\n",
        "kfold_loss_t.append(np.array(batch_loss_t).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}